X <- read.csv("./data/X_.csv", header = TRUE)
y <- read.csv("./data/y_.csv", header = TRUE)
data <- cbind(X, y)
set.seed(123) # 123
indices <- 1:nrow(data)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # 修改比例
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
# Load the e1071 library
library(e1071)
# Train a Linear SVM model
linear_svm_model <- svm(dod ~ ., data = train_data, kernel = "linear")
# Make predictions using the trained models
predictions <- predict(linear_svm_model, newdata = test_data)
confusion_matrix <- table(as.numeric(test_y), as.numeric(ifelse(predictions > 0.5, 1, 0)))
data <- cbind(X, y)
set.seed(123) # 123
indices <- 1:nrow(data)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # 修改比例
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
# Load the e1071 library
library(e1071)
# Train a Linear SVM model
linear_svm_model <- svm(dod ~ ., data = train_data, kernel = "linear")
# Make predictions using the trained models
predictions <- predict(linear_svm_model, newdata = test_data)
confusion_matrix <- table(as.numeric(test_y), as.numeric(ifelse(predictions > 0.5, 1, 0)))
## 计算 Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算 Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## 计算 F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Linear SVM",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
# Load the e1071 library
library(e1071)
set.seed(123) # Set seed for reproducibility
# Combine X and y into a single dataframe
data <- cbind(X, y)
# Perform 10-fold cross-validation
num_folds <- 10
folds <- cut(seq(1, nrow(data)), breaks = num_folds, labels = FALSE)
# Create empty vectors to store the predictions and actual values
all_predictions <- vector()
all_actuals <- vector()
for (i in 1:num_folds) {
# Split the data into training and test sets for the current fold
train_data <- data[folds != i, ]
test_data <- data[folds == i, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
dtrain <- xgb.DMatrix(data = train_X, label = train_y)
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
dtest <- xgb.DMatrix(data = test_X, label = test_y)
# Train a Linear SVM model
linear_svm_model <- svm(dod ~ ., data = train_data, kernel = "linear")
# Make predictions using the trained models
predictions <- predict(linear_svm_model, newdata = test_data)
# Append the predictions and actual values to the vectors
all_predictions <- c(all_predictions, predictions)
all_actuals <- c(all_actuals, test_y)
}
# Load the e1071 library
library(e1071)
set.seed(123) # Set seed for reproducibility
# Combine X and y into a single dataframe
data <- cbind(X, y)
# Perform 10-fold cross-validation
num_folds <- 10
folds <- cut(seq(1, nrow(data)), breaks = num_folds, labels = FALSE)
# Create empty vectors to store the predictions and actual values
all_predictions <- vector()
all_actuals <- vector()
for (i in 1:num_folds) {
# Split the data into training and test sets for the current fold
train_data <- data[folds != i, ]
test_data <- data[folds == i, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
# Train a Linear SVM model
linear_svm_model <- svm(dod ~ ., data = train_data, kernel = "linear")
# Make predictions using the trained models
predictions <- predict(linear_svm_model, newdata = test_data)
# Append the predictions and actual values to the vectors
all_predictions <- c(all_predictions, predictions)
all_actuals <- c(all_actuals, test_y)
}
confusion_matrix <- table(as.numeric(test_y), as.numeric(ifelse(predictions > 0.5, 1, 0)))
## 计算 Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算 Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## 计算 F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
# Calculate ROC curve using the actual values and predictions
roc_obj <- roc(all_actuals, all_predictions)
# Plot the ROC curve
plot(
roc_obj,
col = "blue",
main = "ROC Curve - Linear SVM (Cross-Validation)",
legacy.axes = TRUE,
print.auc = TRUE,
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("green", "orange")
)
load("~/Documents/GitHub/IBD-EDA/r scripts/.RData")
data <- cbind(X, y)
set.seed(123) # 123
indices <- 1:nrow(data)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # 修改比例
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
# Load the e1071 library
library(e1071)
# Train a Cubic SVM model
cubic_svm_model <- svm(dod ~ ., data = train_data, kernel = "polynomial", degree = 3)
# Make predictions using the trained models
predictions <- predict(linear_svm_model, newdata = test_data)
confusion_matrix <- table(as.numeric(test_y), as.numeric(ifelse(predictions > 0.5, 1, 0)))
## 计算 Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算 Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## 计算 F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Linear SVM",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Cubic SVM",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
X <- read.csv("./data/X_.csv", header = TRUE)
y <- read.csv("./data/y_.csv", header = TRUE)
# Load the e1071 library
library(e1071)
set.seed(123) # Set seed for reproducibility
# Combine X and y into a single dataframe
data <- cbind(X, y)
# Perform 10-fold cross-validation
num_folds <- 10
folds <- cut(seq(1, nrow(data)), breaks = num_folds, labels = FALSE)
# Create empty vectors to store the predictions and actual values
all_predictions <- vector()
all_actuals <- vector()
for (i in 1:num_folds) {
# Split the data into training and test sets for the current fold
train_data <- data[folds != i, ]
test_data <- data[folds == i, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
# Train a Cubic SVM model
cubic_svm_model <- svm(dod ~ ., data = train_data, kernel = "polynomial", degree = 3)
# Make predictions using the trained models
predictions <- predict(cubic_svm_model, newdata = test_data)
# Append the predictions and actual values to the vectors
all_predictions <- c(all_predictions, predictions)
all_actuals <- c(all_actuals, test_y)
}
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
unlink("cubic_svm_kfold_cache", recursive = TRUE)
X <- read.csv("./data/X_.csv", header = TRUE)
y <- read.csv("./data/y_.csv", header = TRUE)
data <- cbind(X, y)
set.seed(123) # 123
indices <- 1:nrow(data)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # 修改比例
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
X <- read.csv("./data/X_.csv", header = TRUE)
y <- read.csv("./data/y_.csv", header = TRUE)
data <- cbind(X, y)
set.seed(123) # 123
indices <- 1:nrow(data)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # 修改比例
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
library(randomForest)
# Train the Bagged Tree model
bagged_tree <- randomForest(train_X, train_y, ntree = 100)
# Make predictions on the test set
predictions <- predict(bagged_tree, test_X)
# Continue with the Performance and ROC Curve sections in your code
confusion_matrix <- table(as.numeric(test_y), as.numeric(ifelse(predictions > 0.5, 1, 0)))
## 计算 Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算 Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## 计算 F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Bagged Tree",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
X <- read.csv("./data/X_.csv", header = TRUE)
y <- read.csv("./data/y_.csv", header = TRUE)
library(randomForest)
set.seed(123) # Set seed for reproducibility
# Combine X and y into a single dataframe
data <- cbind(X, y)
# Perform 10-fold cross-validation
num_folds <- 10
folds <- cut(seq(1, nrow(data)), breaks = num_folds, labels = FALSE)
# Create empty vectors to store the predictions and actual values
all_predictions <- vector()
all_actuals <- vector()
for (i in 1:num_folds) {
# Split the data into training and test sets for the current fold
train_data <- data[folds != i, ]
test_data <- data[folds == i, ]
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
# Train the Bagged Tree model
bagged_tree <- randomForest(train_X, train_y, ntree = 100)
# Make predictions on the test set
predictions <- predict(bagged_tree, test_X)
# Append the predictions and actual values to the vectors
all_predictions <- c(all_predictions, predictions)
all_actuals <- c(all_actuals, test_y)
}
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
# Calculate ROC curve using the actual values and predictions
roc_obj <- roc(all_actuals, all_predictions)
# Plot the ROC curve
plot(
roc_obj,
col = "blue",
main = "ROC Curve - Bagged Tree (Cross-Validation)",
legacy.axes = TRUE,
print.auc = TRUE,
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("green", "orange")
)
