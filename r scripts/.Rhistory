all_actuals <- c(all_actuals, test_y)
}
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
# Calculate ROC curve using the actual values and predictions
roc_obj <- roc(all_actuals, all_predictions)
# Plot the ROC curve
plot(
roc_obj,
col = "blue",
main = "ROC Curve - Bagged Tree (Cross-Validation)",
legacy.axes = TRUE,
print.auc = TRUE,
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("green", "orange")
)
X <- read.csv("./data/X__.csv", header = TRUE)
y <- read.csv("./data/y__.csv", header = TRUE)
View(X)
View(X)
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
library('MASS')
library('caret')
new_formula <- dod ~ age + X25000 + X2639 + X2761 + X2762 + X2768 + X27800 + X2875 + X3051 + X311 + X32723 + X4019 + X40390 + X412 + X4280 + X496 + X51881 + X5589 + X56089 + X56400 + X56722 + X5990 + X78659 + X99592 + X99859 + D649 + E8490 + F419 + K219 + N179 + V1582 + V442 + V5866 + Z9049
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
library('MASS')
library('caret')
# new_formula <- dod ~ age + X25000 + X2639 + X2761 + X2762 + X2768 + X27800 + X2875 + X3051 + X311 + X32723 + X4019 + X40390 + X412 + X4280 + X496 + X51881 + X5589 + X56089 + X56400 + X56722 + X5990 + X78659 + X99592 + X99859 + D649 + E8490 + F419 + K219 + N179 + V1582 + V442 + V5866 + Z9049
# new_model <- glm(formula = new_formula, family = binomial, data = train_data)
# summary_model <- summary(new_model)
# step_model <- new_model
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
step_model
library('MASS')
library('caret')
# new_formula <- dod ~ age + X25000 + X2639 + X2761 + X2762 + X2768 + X27800 + X2875 + X3051 + X311 + X32723 + X4019 + X40390 + X412 + X4280 + X496 + X51881 + X5589 + X56089 + X56400 + X56722 + X5990 + X78659 + X99592 + X99859 + D649 + E8490 + F419 + K219 + N179 + V1582 + V442 + V5866 + Z9049
new_formula <- dod ~ age0 + X25000 + X2639 + X2761 + X2762 + X2768 + X27800 + X2875 + X3051 + X311 + X32723 + X4019 + X40390 + X412 + X4280 + X51881 + X5589 + X56089 + X56722 + X5990 + X73300 + X78321 + X7850 + X78659 + X99592 + X99859 + D649 + E8490 + K219 + N179 + V1582 + V442 + Y929 + Z9049
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
step_model <- new_model
# logit_model <- glm(dod ~ ., data = train_data, family = binomial)
# step_model <- stepAIC(logit_model, direction = "both")
summary.glm(step_model)
ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
X <- read.csv("./data/X__.csv", header = TRUE)
y <- read.csv("./data/y__.csv", header = TRUE)
X <- read.csv("./data/X__.csv", header = TRUE)
y <- read.csv("./data/y__.csv", header = TRUE)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
new_formula <- update(new_formula, . ~ . - X5589)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
new_formula <- update(new_formula, . ~ . - X7850)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - E8490)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X78321)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X2639)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - Y929)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - Z9049)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X4019)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - V442)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - V1582)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X73300)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X2768)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X56722)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X99859)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X78659)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_model
summary_model
new_formula
summary.glm(new_model)
ci <- confint(new_model)
exp(cbind(OR <- coef(new_model), ci))
print(new_formula)
X <- read.csv("./data/X__.csv", header = TRUE)
y <- read.csv("./data/y__.csv", header = TRUE)
# new_formula <- dod ~ age + X25000 + X2761 + X2762 + X27800 + X2875 + X3051 + X311 + X32723 + X412 + X4280 + X51881 + X56089 + X5990 + X99592 + D649 + F419 + N179 + V1582
new_formula <- dod ~ age0 + X25000 + X2761 + X2762 + X27800 + X2875 + X3051 + X311 + X32723 + X40390 + X412 + X4280 + X51881 + X56089 + X5990 + X99592 + D649 + K219 + N179
set.seed(123) # Set seed for reproducibility
# Combine X and y into a single dataframe
data <- cbind(X, y)
# Perform 10-fold cross-validation
num_folds <- 10
folds <- cut(seq(1, nrow(data)), breaks = num_folds, labels = FALSE)
# Create empty vectors to store the predictions and actual values
all_predictions <- vector()
all_actuals <- vector()
for (i in 1:num_folds) {
# Split the data into training and test sets for the current fold
train_data <- data[folds != i, ]
test_data <- data[folds == i, ]
# Logistic Regression
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
predictions <- predict(new_model, test_data, type="response")
# Append the predictions and actual values to the vectors
all_predictions <- c(all_predictions, predictions)
all_actuals <- c(all_actuals, test_data[, ncol(test_data)])
}
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
# Calculate ROC curve using the actual values and predictions
roc_obj <- roc(all_actuals, all_predictions)
# Plot the ROC curve
plot(
roc_obj,
col = "blue",
main = "ROC Curve - Logistic Regression (Cross-Validation)",
legacy.axes = TRUE,
print.auc = TRUE,
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("green", "orange")
)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
confusion_matrix
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TN <- confusion_matrix[2, 2]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
## Calculate Specificity
TP <- confusion_matrix[2, 2]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 2]
FN <- confusion_matrix[2, 2]
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
(TP + TN) / (TP + FP + TN + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
(TP + TN) / (TP + FP + TN + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
# precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
TP / (TP + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
# precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
a <- c(1, 1, 1, 0)
b <- c(1, 0, 0, 1)
table(a, b)
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
precision
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
precision
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
(TP + TN) / (TP + FP + TN + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
# precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
(TP + TN) / (TP + TN + FP + FN)
TP / (TP + FP)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate Accuracy
accuracy <- (TP + TN) / (TP + FP + TN + FN)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- TP / (TP + FN)
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- TP / (TP + FP)
cat("Precision:", precision, "\n")
## Calculate Specificity
specificity <- TN / (TN + FP)
cat("Specificity:", specificity, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate Accuracy
accuracy <- (TP + TN) / (TP + FP + TN + FN)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- TP / (TP + FN)
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- TP / (TP + FP)
cat("Precision:", precision, "\n")
## Calculate Specificity
specificity <- TN / (TN + FP)
cat("Specificity:", specificity, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
