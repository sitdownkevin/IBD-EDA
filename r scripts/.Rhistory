ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
# Performance
confusion_matrix <- table(test_data[, ncol(test_data)], predictions)
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# ROC Curve
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
source("~/Repo/IBD-EDA/r scripts/logistic_regression_dod.R")
source("~/Repo/IBD-EDA/r scripts/logistic_regression_dod.R")
source("~/Repo/IBD-EDA/r scripts/xgboost_dod.R")
source("~/Repo/IBD-EDA/r scripts/xgboost_dod.R")
# 模型评价
library(pROC)
roc_obj <- roc(test_y, predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - XGBoost",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
source("~/Repo/IBD-EDA/r scripts/logistic_regression_dod.R")
# 加载数据
X <- read.csv("./data/X.csv", header = TRUE)
source("~/Repo/IBD-EDA/r scripts/logistic_regression_dod.R")
source("~/Repo/IBD-EDA/r scripts/logistic_regression_dod.R")
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
# 划分训练集和测试集
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
# Logistic Regression using stepAIC
library('MASS')
library('caret')
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
View(X)
View(X)
View(X)
View(X)
View(X)
View(X)
View(X)
summary.glm(step_model)
ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
# Performance
confusion_matrix <- table(test_data[, ncol(test_data)], predictions)
confusion_matrix
# Performance
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
confusion_matrix
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# ROC Curve
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
# 加载数据
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
source("~/Repo/IBD-EDA/r scripts/logistic_regression_dod.R")
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
install.packages(c("cli", "data.table", "e1071", "fansi", "recipes", "stringi", "timeDate"))
gc()
libPaths()
.libPaths()
install.packages('IRkernel')
IRkernel::installspec()
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
gc()
gc()
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
View(X)
View(X)
set.seed(123)
train_size <- floor(0.8 * length(indices))
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
# 划分训练集和测试集
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
# Logistic Regression using stepAIC
library('MASS')
library('caret')
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
data <- cbind(X, y)
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.8 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
library('MASS')
library('caret')
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
summary.glm(step_model)
ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.8 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
library('MASS')
library('caret')
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
summary.glm(step_model)
ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
View(X)
View(y)
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.8 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
library('MASS')
library('caret')
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
summary.glm(step_model)
ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123) # <=
shuffled_indices <- sample(indices)
train_size <- floor(0.8 * length(indices)) # <=
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
library('MASS')
library('caret')
logit_model <- glm(dod ~ ., data = train_data, family = binomial)
step_model <- stepAIC(logit_model, direction = "both")
summary.glm(step_model)
ci <- confint(step_model)
exp(cbind(OR <- coef(step_model), ci))
predictions <- predict(step_model, test_data, type="response")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
gc()
X <- read.csv("./data/X.csv", header = TRUE)
y <- read.csv("./data/y.csv", header = TRUE)
data <- cbind(X, y)
set.seed(123) # 123
indices <- 1:nrow(data)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices)) # 修改比例
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
library(xgboost)
train_X <- as.matrix(train_data[, -ncol(train_data)])
train_y <- train_data[, ncol(train_data)]
dtrain <- xgb.DMatrix(data = train_X, label = train_y)
test_X <- as.matrix(test_data[, -ncol(test_data)])
test_y <- test_data[, ncol(test_data)]
dtest <- xgb.DMatrix(data = test_X, label = test_y)
xgb_model <- xgboost(data = dtrain, nrounds = 10, objective = "binary:logistic")
predictions <- predict(xgb_model, dtest)
confusion_matrix <- table(as.numeric(test_y), as.numeric(ifelse(predictions > 0.5, 1, 0)))
## 计算 Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算 Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## 计算 F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
main = "ROC Curve - XGBoost",
xlab = "False Positive Rate (FPR)",
ylab = "True PositiveRate (TPR)",
legacy.axes = TRUE,
print.auc = TRUE,
auc.polygon = TRUE,
auc.polygon.col = "lightblue",
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("blue","blue"),
# xlim = c(0, 1),
# ylim = c(0, 1),
asp = 1
)
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
plot(
roc_obj,
main = "ROC Curve - XGBoost",
xlab = "False Positive Rate (FPR)",
ylab = "True PositiveRate (TPR)",
legacy.axes = TRUE,
print.auc = TRUE,
auc.polygon = TRUE,
auc.polygon.col = "lightblue",
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("blue","yellow"),
# xlim = c(0, 1),
# ylim = c(0, 1),
asp = 1
)
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
<!--plot(
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
main = "ROC Curve - XGBoost",
xlab = "False Positive Rate (FPR)",
ylab = "True PositiveRate (TPR)",
legacy.axes = TRUE,
print.auc = TRUE,
auc.polygon = TRUE,
auc.polygon.col = "lightblue",
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("blue","yellow"),
# xlim = c(0, 1),
# ylim = c(0, 1),
asp = 1
)
library(pROC)
roc_obj <- roc(as.numeric(test_y),
as.numeric(predictions),)
plot(
roc_obj,
col = "red",
main = "ROC Curve - Logistic Regression",
legacy.axes = T, # y轴格式更改
print.auc = TRUE, # 显示AUC面积
print.thres = TRUE, # 添加截点和95%CI
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
# 构建ANN模型
library(neuralnet)
install.packages('neuralnet')
y <- read.csv("./data/y.csv", header = TRUE)
# 加载数据
X <- read.csv("./data/X.csv", header = TRUE)
data <- cbind(X, y)
indices <- 1:nrow(data)
set.seed(123)
shuffled_indices <- sample(indices)
train_size <- floor(0.7 * length(indices))
train_indices <- shuffled_indices[1:train_size]
test_indices <- shuffled_indices[(train_size + 1):length(indices)]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
test_data <- data[test_indices, ]
# 构建ANN模型
library(neuralnet)
# 设计ANN模型结构
ann_formula <- as.formula("dod ~ .")
ann_model <- neuralnet(ann_formula, data = train_data, hidden = c(60, 30, 10))
# 模型训练
trained_model <- ann_model
# 预测
predictions <- compute(trained_model, test_data[, -ncol(test_data)])$net.result
predicted_classes <- ifelse(predictions > threshold, 1, 0)
# Performance
threshold <- 0.5
predicted_classes <- ifelse(predictions > threshold, 1, 0)
# 计算准确率
accuracy <- sum(predicted_classes == test_data[, ncol(test_data)]) / nrow(test_data)
cat("Accuracy:", accuracy, "\n")
# 模型训练
trained_model <- ann_model
# 计算召回率
recall <- sum(predicted_classes == 1 & test_data[, ncol(test_data)] == 1) / sum(test_data[, ncol(test_data)] == 1)
cat("Recall:", recall, "\n")
# 计算F1分数
precision <- sum(predicted_classes == 1 & test_data[, ncol(test_data)] == 1) / sum(predicted_classes == 1)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# ROC Curve
library(pROC)
roc_obj <- roc(test_data[, ncol(test_data)], predictions)
plot(
roc_obj,
col = "red",
main = "ROC Curve - ANN",
legacy.axes = T,
print.auc = TRUE,
print.thres = TRUE,
grid=c(0.2,0.2),
grid.col=c("blue","yellow")
)
