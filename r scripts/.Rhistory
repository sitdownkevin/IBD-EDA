X <- read.csv("./data/X__.csv", header = TRUE)
y <- read.csv("./data/y__.csv", header = TRUE)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
new_formula <- update(new_formula, . ~ . - X5589)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
new_formula <- update(new_formula, . ~ . - X7850)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - E8490)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X78321)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X2639)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - Y929)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - Z9049)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X4019)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - V442)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - V1582)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X73300)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X2768)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X56722)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X99859)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_formula <- update(new_formula, . ~ . - X78659)
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
summary_model <- summary(new_model)
p_values <- summary_model$coefficients[, "Pr(>|z|)"]
max_p_value_index <- which.max(p_values)
max_p_value_attribute <- names(p_values)[max_p_value_index]
print(max_p_value_attribute)
print(max(p_values))
new_model
summary_model
new_formula
summary.glm(new_model)
ci <- confint(new_model)
exp(cbind(OR <- coef(new_model), ci))
print(new_formula)
X <- read.csv("./data/X__.csv", header = TRUE)
y <- read.csv("./data/y__.csv", header = TRUE)
# new_formula <- dod ~ age + X25000 + X2761 + X2762 + X27800 + X2875 + X3051 + X311 + X32723 + X412 + X4280 + X51881 + X56089 + X5990 + X99592 + D649 + F419 + N179 + V1582
new_formula <- dod ~ age0 + X25000 + X2761 + X2762 + X27800 + X2875 + X3051 + X311 + X32723 + X40390 + X412 + X4280 + X51881 + X56089 + X5990 + X99592 + D649 + K219 + N179
set.seed(123) # Set seed for reproducibility
# Combine X and y into a single dataframe
data <- cbind(X, y)
# Perform 10-fold cross-validation
num_folds <- 10
folds <- cut(seq(1, nrow(data)), breaks = num_folds, labels = FALSE)
# Create empty vectors to store the predictions and actual values
all_predictions <- vector()
all_actuals <- vector()
for (i in 1:num_folds) {
# Split the data into training and test sets for the current fold
train_data <- data[folds != i, ]
test_data <- data[folds == i, ]
# Logistic Regression
new_model <- glm(formula = new_formula, family = binomial, data = train_data)
predictions <- predict(new_model, test_data, type="response")
# Append the predictions and actual values to the vectors
all_predictions <- c(all_predictions, predictions)
all_actuals <- c(all_actuals, test_data[, ncol(test_data)])
}
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate F1 Score
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
library(pROC)
# Calculate ROC curve using the actual values and predictions
roc_obj <- roc(all_actuals, all_predictions)
# Plot the ROC curve
plot(
roc_obj,
col = "blue",
main = "ROC Curve - Logistic Regression (Cross-Validation)",
legacy.axes = TRUE,
print.auc = TRUE,
print.thres = TRUE,
grid = c(0.2, 0.2),
grid.col = c("green", "orange")
)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0)))
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
confusion_matrix
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TN <- confusion_matrix[2, 2]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
## Calculate Specificity
TP <- confusion_matrix[2, 2]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 2]
FN <- confusion_matrix[2, 2]
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
(TP + TN) / (TP + FP + TN + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
(TP + TN) / (TP + FP + TN + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
# precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
TP / (TP + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
# precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
a <- c(1, 1, 1, 0)
b <- c(1, 0, 0, 1)
table(a, b)
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[1, 2]
FN <- confusion_matrix[2, 1]
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
precision
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
precision
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
(TP + TN) / (TP + FP + TN + FN)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- diag(confusion_matrix)[1] / sum(confusion_matrix[1,])
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- (TP + TN) / (TP + FP + TN + FN)
# precision <- diag(confusion_matrix)[1] / colSums(confusion_matrix)[1]
cat("Precision:", precision, "\n")
## Calculate Specificity
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
(TP + TN) / (TP + TN + FP + FN)
TP / (TP + FP)
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(
as.numeric(all_actuals), as.numeric(ifelse(all_predictions > 0.5, 1, 0))
)
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate Accuracy
accuracy <- (TP + TN) / (TP + FP + TN + FN)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- TP / (TP + FN)
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- TP / (TP + FP)
cat("Precision:", precision, "\n")
## Calculate Specificity
specificity <- TN / (TN + FP)
cat("Specificity:", specificity, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
## 计算准确率
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
## 计算召回率
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
cat("Recall:", recall, "\n")
## 计算F1分数
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# Calculate performance metrics on the entire dataset
confusion_matrix <- table(test_data[, ncol(test_data)], ifelse(predictions > 0.5, 1, 0))
TP <- confusion_matrix[1, 1]
TN <- confusion_matrix[2, 2]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]
## Calculate Accuracy
accuracy <- (TP + TN) / (TP + FP + TN + FN)
cat("Accuracy:", accuracy, "\n")
## Calculate Recall
recall <- TP / (TP + FN)
cat("Recall:", recall, "\n")
## Calculate Precision
precision <- TP / (TP + FP)
cat("Precision:", precision, "\n")
## Calculate Specificity
specificity <- TN / (TN + FP)
cat("Specificity:", specificity, "\n")
## Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
# X <- read.csv("./data/X__.csv", header = TRUE)
# y <- read.csv("./data/y__.csv", header = TRUE)
data <- read.csv('../data/data.csv')
View(data)
View(data)
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
library(dplyr)
View(data)
View(data)
data %>%
select(-subject_id, -dod)
data %>%
select(-subject_id, -dod) %>%
colnames()
data %>%
select(dod) ->
y
data %>%
select(-subject_id, -dod) ->
X
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
data <- read.csv('./data/data_kept.csv')
View(data)
View(data)
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
colnames(data)
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
data %>%
select(-dod, -subject_id)
data %>%
select(-dod, -subject_id)
View(data)
View(data)
data %>%
select(-dod, -subject_id, -X)
data %>%
select(-dod, -subject_id, -X) %>%
data %>%
select(-c(dod, subject_id, X)) %>%
data %>%
select(-c(dod, subject_id, X))
data %>%
select(-c(dod, subject_id))
data %>%
select(-c(dod))
source("~/GitHub/IBD-EDA/new r scripts/logistic_regression.R", echo=TRUE)
data %<%
select(dod)
data %>%
select(dod)
data <- read.csv('./data/data_kept.csv')
data %>%
select(dod)
read.csv('./data/data_kept.csv') ->
data
data
data %>%
select(dod)
names(data)
read.csv('./data/data_kept.csv') ->
select(-c("X"))
read.csv('./data/data_kept.csv') %>%
select(-c("X")) -> data
read.csv('./data/data_kept.csv') %>%
select(-"X") -> data
read.csv('./data/data_kept.csv') %>%
select(-X) -> data
read.csv('./data/data_kept.csv') %>%
select(-dod) -> data
read.csv('./data/data_kept.csv') %>%
select(dod) -> data
setwd('~/GitHub/IBD-EDA')
library(dplyr)
read.csv('./data/data_kept.csv') %>%
select(dod) -> data
read.csv('./data/data_kept.csv')
read.csv('./data/data_kept.csv') -> data
data
data %>%
colnames()
data[,dod]
data[,c(dod)]
data[,c('dod')]
data %>%
select("dod")
data %>%
select(, "dod")
data %>%
select(dod)
