---
title: "XGBoost"
output: 
  html_notebook:
    toc: true
    theme: cosmo
---

# Installing Packages

```{r}
rm(list=ls(all=TRUE))
# setwd('~/GitHub/IBD-EDA/aes/')
setwd('E:/Project/IBD-EDA/aes/')
```

```{r, echo=FALSE}
library(dplyr)
library(xgboost)
library(performance)
library(ggplot2)
library(corrplot)
library(DALEX)
library(caret)
```

# Loading Data

```{r}
data <- read.csv('./data_processed/data.csv') %>%
  select(-X)
```

```{r}
set.seed(123)
train_ratio = 0.8

train_indices <- sample(1:nrow(data), size = floor(train_ratio * nrow(data)))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

```{r}
dtrain <- xgb.DMatrix(
  data = as.matrix(train_data[,-1]), label = train_data[,1]
)

dtest <- xgb.DMatrix(
  data = as.matrix(test_data[,-1]), label = test_data[,1]
)

```

# Find Parameters

```{r}
grid <- expand.grid(
  nrounds = c(50, 100, 150), # 添加nrounds参数
  max_depth = c(3, 4, 5, 6),
  min_child_weight = c(1, 2, 3),
  eta = c(0.05, 0.1, 0.3),
  gamma = c(0, 0.1, 0.2),
  subsample = c(0.7, 0.8, 0.9),
  colsample_bytree = c(0.7, 0.8, 0.9)
)

# 训练控制
trainControl <- trainControl(
  method = "cv", 
  number = 5,
  allowParallel = TRUE, # 允许并行处理
  verboseIter = FALSE # 减少训练过程中的输出，使输出更清晰
)

# 训练模型
model <- train(
  x = as.matrix(train_data[,-1]), y = train_data[,1],
  method = "xgbTree",
  trControl = trainControl,
  tuneGrid = grid,
  metric = "RMSE"
)

print(model$bestTune)

```