---
title: "XGBoost"
output: 
  html_notebook:
    toc: true
    theme: cosmo
---

# Installing Packages

```{r}
rm(list=ls(all=TRUE))
# setwd('~/GitHub/IBD-EDA/aes/')
setwd('E:/Project/IBD-EDA/aes/')
```

```{r, echo=FALSE}
library(dplyr)
library(xgboost)
library(performance)
library(ggplot2)
library(corrplot)
library(DALEX)
library(caret)
```

# Loading Data

```{r}
data <- read.csv('./data_processed/data.csv') %>%
  select(-X)
```

```{r}
set.seed(123)
train_ratio = 0.8

train_indices <- sample(1:nrow(data), size = floor(train_ratio * nrow(data)))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

```{r}
dtrain <- xgb.DMatrix(
  data = as.matrix(train_data[,-1]), label = train_data[,1]
)

dtest <- xgb.DMatrix(
  data = as.matrix(test_data[,-1]), label = test_data[,1]
)

```

# Find Parameters

```{r}
grid <- expand.grid(
  nrounds = c(50, 100, 150), # 添加nrounds参数
  max_depth = c(3, 4, 5, 6),
  min_child_weight = c(1, 2, 3),
  eta = c(0.05, 0.1, 0.3),
  gamma = c(0, 0.1, 0.2),
  subsample = c(0.7, 0.8, 0.9),
  colsample_bytree = c(0.7, 0.8, 0.9)
)

# 训练控制
trainControl <- trainControl(
  method = "cv", 
  number = 5,
  allowParallel = TRUE, # 允许并行处理
  verboseIter = FALSE # 减少训练过程中的输出，使输出更清晰
)

# 训练模型
model <- train(
  x = as.matrix(train_data[,-1]), y = train_data[,1],
  method = "xgbTree",
  trControl = trainControl,
  tuneGrid = grid,
  metric = "RMSE"
)

print(model$bestTune)

```

# XGBoost

```{r}
nrounds <- 50
params <- list(
  objective = "reg:squarederror",
  max_depth = 3,
  min_child_weight = 2,
  eta = 0.05,
  gamma = 0.1,
  subsample = 0.7,
  colsample_bytree = 0.8
)

final_model <- xgboost(
  data = dtrain, 
  params = params, 
  nrounds = nrounds, 
  print_every_n = 10, 
  early_stopping_rounds = 10, 
  eval_metric = "rmse", 
  evals = list(validation = dtest)
)

```

# Model Evaluation

```{r}
actuals_test <- test_data[,1]
preds_test <- predict(final_model, newdata = as.matrix(test_data[,-1]))

results <- postResample(pred = preds_test, obs = actuals_test)

print(paste("RMSE on Test Set: ", results[1]))
print(paste("MAE on Test Set: ", results[2]))
print(paste("R2 on Test Set: ", results[3]))

```

# Model Results

## Importance Matrix

```{r}
importance_matrix <- xgb.importance(
  feature_names = colnames(train_data[, -1]), 
  model = final_model
)

xgb.plot.importance(importance_matrix)
```

## 实际值与预测值比较

```{r}
comparison_df <- data.frame(
  Actual = actuals_test, 
  Prediction = preds_test
)

ggplot(comparison_df, aes(x = Actual, y = Prediction)) +
  geom_point(colour = "blue") +  # 绘制散点
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +  # 添加等值线
  theme_minimal() +  # 使用简洁主题
  labs(title = "Actual vs. Prediction", x = "Actual Value", y = "Prediction Value") +  # 添加图标题和轴标题
  theme(plot.title = element_text(hjust = 0.5))  # 居中标题

```

## Residual Analysis

```{r}
residuals <- actuals_test - preds_test

ggplot() + 
  geom_histogram(aes(x=residuals), binwidth = 0.2, fill="blue", color="black") + 
  ggtitle("Residuals Distribution") +
  xlab("Residuals") +
  ylab("Frequency")

```

```{r}
ggplot() +
  geom_point(aes(x=preds_test, y=residuals), color="red") +
  ggtitle("Residuals vs. Predicted Values") +
  xlab("Predicted Values") +
  ylab("Residuals") +
  geom_hline(yintercept=0, linetype="dashed", color = "blue")

```